<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Reflexiv</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Reflexiv</h1>
      <h2 class="project-tagline">Reflexiv - a fast, scalable, distributed <i>De novo</i> genome assembler</h2>
      <a href="index.html" class="btn">Home page</a>
      <a href="example.html" class="btn">Getting started</a>
      <a href="manual.html" class="btn">User manual</a>
      <a href="javadoc/index.html" class="btn">&nbsp;&nbsp;Javadoc&nbsp;&nbsp;&nbsp;</a>
        <br>
      <a href="https://github.com/rhinempi/Reflexiv/archive/latest.zip" class="btn">&nbsp;Download&nbsp;</a>
      <a href="https://rhinempi.github.io/sparkhit/" class="btn">Portal: Sparkhit</a>
      <a href="https://github.com/rhinempi/Reflexiv" class="btn">Source code&nbsp;</a>
      <a href="https://github.com/rhinempi/Reflexiv/issues" class="btn">Questions&nbsp;</a>
    </section>
    
<div id="mySidenav" class="sidenav">
  <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">×</a>
  <a href="#Overview" style="font-size:16px">Overview</a>
  <a href="#Command" style="font-size:16px">Command</a>
  <a href="#Spark-options" style="font-size:16px" >Spark options</a>
  <a href="#run-options"style="font-size:16px" >Reflexiv run</a>
  <a href="#counter-options"style="font-size:16px" >Reflexiv counter</a>
  <a href="#reassembler-options" style="font-size:16px">Reflexiv reassembler</a>
    <a href="#setting-spark-sge" style="font-size:16px">Setting Spark on SGE</a>
    <a href="#setting-spark-ec2" style="font-size:16px">Setting Spark on EC2</a>
    <a href="#" style="font-size:16px">Back to top</a>
</div>

<div id="main">
  <span style="font-size:30px;cursor:pointer" onclick="openNav()">☰ Menu</span>
</div>

<script>
function openNav() {
    document.getElementById("mySidenav").style.width = "250px";
    document.getElementById("main").style.marginLeft = "250px";
    document.body.style.backgroundColor = "white";
}

function closeNav() {
    document.getElementById("mySidenav").style.width = "0";
    document.getElementById("main").style.marginLeft= "0";
    document.body.style.backgroundColor = "white";
}
</script>

    <section class="main-content">
      
      <h3>
<a id="Overview" class="anchor" href="#Overview" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h3>

        <p>
            <i>Reflexiv</i> command line options consist of four parts <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>spark-submit</b> + <b>[spark-options]</b> +  <b>Reflexiv.jar</b> + <b>[Reflexiv-options]</b>. <br>
            <i>Reflexiv</i> executable file wraps the command and simplifies as <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>Reflexiv</b> + <b>[command]</b> + <b>[Spark options]</b> + <b>[Reflexiv options]</b> <br>
            where [command] specifies a particular function of the assembler; [Spark options] are parameters for the <i>Spark</i> framework, eg. to configure the <i>Spark</i> cluster; [Reflexiv options] are parameters for <i>Reflexiv</i>.
        </p>
        <p>
            The examples below compare the two types of input commands:
        </p>
        <pre>
#Created by rhinempi on 23/12/17.
 #
 #      Reflexiv
 #
 # Copyright (c) 2015-2015
 #      Liren Huang      &lt;huanglr at cebitec.uni-bielefeld.de>
 # 
 # Reflexiv is free software: you can redistribute it and/or modify it
 # under the terms of the GNU General Public License as published by the Free
 # Software Foundation, either version 3 of the License, or (at your option)
 # any later version.
 #
 # This program is distributed in the hope that it will be useful, but WITHOUT
 # ANY WARRANTY; Without even the implied warranty of MERCHANTABILITY or
 # FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for
 # more detail.
 # 
 # You should have received a copy of the GNU General Public License along
 # with this program. If not, see &lt;http://www.gnu.org/licenses>.


# path
sparkbin="/root/spark/bin"
Reflexivlib="/mnt/software/Reflexiv/lib/"        
      
# spark submit
time $sparkbin/spark-submit \                                      # spark-submit
	--conf "spark.eventLog.enabled=true" \                     # [spark-options]
	--driver-memory 15G \
	--executor-memory 57G \
	--class uni.bielefeld.cmg.Reflexiv.main.Main \
	$Reflexivlib/original-Reflexiv-0.3.jar \                   # Reflexiv.jar
		-fastq /mnt/HMP/stool/part* \                      # [Reflexiv-options]
		-outfile /mnt/Reflexiv/stool-result \
		-kmer 31 \    
                -partition 3200 \
		> Reflexiv.log 2> Reflexiv.err</pre>
        <p>
            The second one:
        </p>
        <pre>
#Created by rhinempi on 06/01/18.
 #
 #      Reflexiv
 #
 # Copyright (c) 2015-2015
 #      Liren Huang      &lt;huanglr at cebitec.uni-bielefeld.de>
 # 
 # Reflexiv is free software: you can redistribute it and/or modify it
 # under the terms of the GNU General Public License as published by the Free
 # Software Foundation, either version 3 of the License, or (at your option)
 # any later version.
 #
 # This program is distributed in the hope that it will be useful, but WITHOUT
 # ANY WARRANTY; Without even the implied warranty of MERCHANTABILITY or
 # FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for
 # more detail.
 # 
 # You should have received a copy of the GNU General Public License along
 # with this program. If not, see &lt;http://www.gnu.org/licenses>.
 
# path
Reflexiv_HOME="/vol/ec2-user/Reflexiv"

# Reflexiv command
time $Reflexiv_HOME/bin/reflexiv run \            # Reflexiv + [command]
    --driver-memory 3G \                          # [Spark options]
    --executor-memory 3G \
        -fastq ./example/'paired_dat*.fq.gz' \    # [Reflexiv options]
        -outfile ./example/result \
        -kmer 31
        </pre>
        <p>
            
        </p>
<h3>
<a id="Command" class="anchor" href="#Command" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Commands and Modules</h3>

        <p>
            <i>Reflexiv</i> integrates a few functions to faciliate the assembly processes. These functions are activated by using a <b>Command</b> after the <i>Reflexiv</i> executable file. To see all commands and their functions, simply run <code>reflexiv</code>. <br>
            <code>$ ./bin/reflexiv</code>
        </p>
        
        <pre>
Reflexiv - on the cloud.
Version: 0.3

Commands:
  run             Run the entire assembly pipeline
  counter         counting Kmer frequency
  reassembler     re-assemble and extend genome fragments

Type each command to view its options, eg. Usage: ./reflexiv run

Spark cluster configuration:
  --spark-conf       Spark cluster configuration file or spark input parameters
  --spark-param      Spark cluster parameters in quotation marks "--driver-memory 4G --executor-memory 16G"
  --spark-help       View spark-submit options. You can include spark`s options directly.

Usage: reflexiv [commands] --spark-conf spark_cluster_default.conf [option...]
       reflexiv [commands] --spark-param "--driver-memory 4G --executor-memory 16G" [option...]
       reflexiv [commands] --driver-memory 4G --executor-memory 16G --executor-cores 2 [option...]

For detailed cluster submission, please refer to scripts located in:
./sbin</pre>
        <p>
            As shown above, there is a list of commands. Detailed descriptions are as follows:
        </p>
        <table>
            <caption>Table M-1: Descriptions of all Reflexiv commands</caption>
  <tr>
    <th>Command</th>
    <th>Description</th> 
  </tr>
  <tr>
    <td><a href="#run-options">run</a></td>
      <td>This command runs the standard Reflexiv assembly pipeline, from sequencing data to assembled genomes.
      </td> 
  </tr>
  <tr>
    <td><a href="#counter-options">counter</a></td>
      <td>This command counts all K-mer frequencies. This result can be used to analyze the genome composition (GC, K-mer distribution etc.). The result can also be used for tuning the downstream assemblies (assemble the genome with different K-mer sizes or different K-mer coverages).
      </td> 
  </tr>
  <tr>
    <td><a href="#reassembler-options">reassembler</a></td>
    <td>This command reassembles (extends) a collection of sequence fragments using external NGS data (eg. metagenomic datasets).</td> 
  </tr>
</table>
         <p>
             
        </p>
        <div>
            &#9749; &nbsp;Notes
        </div>
        <div-content>
            <ol type="1">
            <li>
                All commands are pointers (not the pointer in the computer programming domain) to a specific main Java class in the program. As shown in the first script above, the <code>--class</code> option passes a main Java class to the <i>Spark</i> framework. Whereas in the second script, the <code>run</code> command assigns the main Java class to <i>Spark</i>.
            </li>
            </ol>
        </div-content>
       
<h3>
<a id="Spark-options" class="anchor" href="#Spark-options" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Spark options</h3>

        <p>Spark options are the options for the <i>Apache Spark</i> framework. You can find detailed explanations of all options at the <i>Spark</i> <a href="http://spark.apache.org/docs/latest/configuration.html">configuration page</a>. </p>
        <p>
            <i>Reflexiv</i> identifies these options through parameters starting with two dashes <code>--</code>, just like the options used in <i>Spark</i>. <i>Reflexiv</i> also provide three additional input options for assigning <i>Spark</i> options. See the table below:
        </p>

        <table>
            <caption>Table M-2: Reflexiv input options for assigning Spark options</caption>
  <tr>
    <th>Option&nbsp;</th>
    <th>Description</th> 
  </tr>
  <tr>
    <td>--spark-conf</td>
      <td>This option specifies a configuration file as input options for <i>Spark</i>. It has the same function as the <code>--properties-file</code> option used in the <code>spark-submit</code> executable.
      </td> 
  </tr>
  <tr>
    <td>--spark-param</td>
      <td>This option reads a series of commandline options quoted within a quotation mark. eg. <code>--spark-param "--driver-memory 2G --executor-memory 4G"</code> passes two parameters <code>--driver-memory</code> and <code>--executor-memory</code> to the <i>Spark</i> framework.</td> 
  </tr>
        <tr>
    <td>--spark-help</td>
            <td>This option invokes <code>spark-submit</code> to print out all available options. This is useful when users want to directly check all the options of <i>Spark</i>.</td> 
  </tr>    
        </table>
        <p>
        
        </p>
         <div>
            &#9749; &nbsp;Notes
        </div>
        <div-content>
            <ol type="1">
            <li>
                In case you are not familiar with <i>Spark</i>, we recommend several important <i>Spark</i> options that users should pay attentions to: <code>--master</code> is important if you want to submit a job in the cluster mode; <code>--driver-memory</code> sets the memory consumption for the main program, e.g. In the <code>Reflexiv-reassembler</code>, driver memory is the memory consumption of building Reflexible Distributed K-mer (RDK) from input fragments; <code>--executor-memory</code> sets the memory for each executor (usually the worker nodes). The memory should be increased accordingly, if users want to<code>cache</code> the input data across cluster nodes.
            </li>
            </ol>
        </div-content>
<h3>
<a id="run-options" class="anchor" href="#run-options" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Reflexiv run options</h3>
        
        <p>
            The <code>run</code> command starts a standard assembly pipeline. The options of the <code>run</code> command mainly consist of input sequencing data (usually a fastq file), a K-mer size and an output result file.
        </p>
        <p>
            Fragment recruitment allows more mismatches during the alignment between sequencing reads and reference genomes. This funtion is useful in situations like evaluating the qualities of metagenomic assemblies, where bad quality matches are also needed for the evaluation.
        </p>
        <p>
            To print out all options, simply type the command: <br>
            <code>$ reflexiv run</code>
        </p>       
        <pre>
Reflexiv 17:11:21 Reflexiv main initiating ... 
Reflexiv 17:11:21 interpreting parameters.
Name:
	Reflexiv Main

Options:
  -fastq &lt;input fastq file>            Input NGS data, fastq file format, four line
                                       per unit
  -fasta &lt;input fasta file>            Also input NGS data, but in fasta file format,
                                       two line per unit
  -outfile &lt;output file>               Output assembly result
  -kmer &lt;kmer size>                    Kmer length for reads mapping
  -overlap &lt;kmer overlap>              Overlap size between two adjacent kmers
  -miniter &lt;minimum iterations>        Minimum iterations for contig construction
  -maxiter &lt;maximum iterations>        Maximum iterations for contig construction
  -clipf &lt;clip front nt>               Clip N number of nucleotides from the
                                       beginning of the reads
  -clipe &lt;clip end nt>                 Clip N number of nucleotides from the end of
                                       the reads
  -cover &lt;minimal kmer coverage>       Minimal coverage to filter low freq kmers
  -maxcov &lt;maximal kmer coverage>      Maximal coverage to filter high freq kmers
  -minlength &lt;minimal read length>     Minimal read length required for assembly
  -mincontig &lt;minimal contig length>   Minimal contig length to be reported
  -partition &lt;re-partition number>     re generate N number of partitions
  -bubble                              Set to NOT remove bubbles.
  -cache                               weather to store data in memory or not
  -version                             show version information
  -h
  -help                                print and show this information

Usage:
	run de novo genome assembly : 
spark-submit [spark parameter] --class uni.bielefeld.cmg.reflexiv.main.Main reflexiv.jar [parameters] -fastq input.fq -kmer 63 -outfile output_file
spark-submit [spark parameter] --class uni.bielefeld.cmg.reflexiv.main.Main reflexiv.jar [parameters] -fasta input.txt -kmer 63 -outfile output_file
reflexiv mapper [spark parameter] [parameters] -fastq input.fq -kmer 63 -outfile output_file"</pre>
        <table>
            <caption>Table M-3:  Descriptions of Reflexiv run options</caption>
  <tr>
    <th>Options</th>
    <th>Description</th> 
  </tr>
  <tr>
    <td>-fastq</td>
      <td>The input sequencing data is in the fastq format. The input files can be compressed or uncompressed. Users can also use the wild card or comma seperated files to input a batch of files. eg. <code>-fastq /vol/data/samples/*.fq.bz2</code>.
      </td> 
  </tr>
  <tr>
    <td>-line</td>
      <td>The input sequencing data is a fastq file in the line-based format. This line-based sequecing data is useful when users have uncompressed files distributed across different nodes. In this case, a fastq unit is intact on one worker node. See below <b>Notes</b>.
      </td> 
  </tr>
        <tr>
    <td>-tag</td>
            <td>Whether to tag the file name to each sequence or not. This tag function can mark each sequence of the intermediate mapping result. This information is useful when users have a list of samples input in the same time and would like to be summarized seperately. This option will increase the size of output file.
            </td> 
  </tr>    
  <tr>
    <td>-reference</td>
            <td>Input file of the reference genome in the fasta format. Make sure it is located in a shared file system or downloaded to each worker node.
            </td> 
  </tr>    
  <tr>
    <td>-outfile</td>
            <td>Directory of output files.
            </td> 
  </tr>
            <tr>
    <td>-kmer</td>
            <td>The K-mer length for building the reference index, 8-12 is recommanded. Default is 12.
            </td> 
  </tr>
            <tr>
    <td>-evalue</td>
            <td>Set a maximum E-value as a filter to remove mappings higher than the threshold.
            </td> 
  </tr>
            <tr>
    <td>-global</td>
            <td>Global alignment or local alignment. Set to 0 as local alignment, to 1 as global alignment. The default is 0.
            </td> 
  </tr>
            <tr>
    <td>-unmask</td>
            <td>Whether to mask repeats in the reference genome or not. 0 for not mask; 1 for mask. The default is 1.
            </td> 
  </tr>
            <tr>
    <td>-overlap</td>
            <td>When extracting k-mers from the reference genome, there will be a shift between k-mers. The k-mer length minus the shifted length is the overlap length. The larger the overlap size is, the more sensitive the mapping result will be.
            </td> 
  </tr>
            <tr>
    <td>-identity</td>
            <td>The mapping identity between the sequencing read and the reference genome. The identity is calculated by dividing perfectly matched nuclieotides to the length of the sequecing read.
            </td> 
  </tr>
            <tr>
    <td>-coverage</td>
            <td>The minimal coverage to recruit a read. Default is 30.
            </td> 
  </tr>
            <tr>
    <td>-minlength</td>
            <td>The minmal length to recruit a read.
            </td> 
  </tr>
            <tr>
    <td>-attempts</td>
            <td>Sparkhit recruiter trys to map each sequence to a collection of q-Grams (a block of sequence that is very likely to be recuited). This parameter sets a number of failed attempts to map to q-Grams. Default is 20
            </td> 
  </tr>
         <tr>
    <td>-hits</td>
            <td>How many hits to be reported in the result for each sequencing read. 0 for report all possible matches. The default is 0.
            </td> 
  </tr>
            <tr>
    <td>-strand</td>
            <td>Strands selection. 0 is for mapping to both strands of reference genome. 1 for mapping to only "+" strand. 2 for mapping to only "-". The Default is 0.
            </td> 
  </tr>
            <tr>
    <td>-thread</td>
            <td>How many threads to be used in parallel. This parameter is deprecated. Check the <code>-partition</code>.
            </td> 
  </tr>
            <tr>
    <td>-partition</td>
            <td>Re-distribute the input data into a number of partitions. Each partition runs a individual mapping task. This is useful when you want to evenly distribute your data for parallel computations.
            </td> 
  </tr>
            <tr>
    <td>-version</td>
            <td>Print out the Sparkhit version.
            </td> 
  </tr>
            <tr>
    <td>-help</td>
            <td>Print the help information.
            </td> 
  </tr>
        </table>
        <p>
        </p>
        <div>
            &#9749; &nbsp;Notes
        </div>
        <div-content>
            <ol type="1">
                <li>
                    Different from the standard short read alignment, fragment recruitment allows more mismatches during the alignment processes between sequencing reads and reference genomes. This function is useful in situations like evaluating the qualities of metagenomic assemblies, where bad quality matches are also needed for the evaluation.
                </li>
            <li>
                As most of the distributed storage systems manage the data distribution automatically (eg. Hadoop distributed file system (HDFS)), uncompressed text files are usually splitted and identified by lines. Fastq files consist of <b>4-line-units</b>: an ID header line starts with "@", a raw sequence line, an identifier starts with "+" and a sequecing quality line. Most tools require the complete information of a fastq unit for their algorithms. So, after decompression, <i>Sparkhit</i> changes a fastq file to a tabular file where each line is a complete unit consists of the four elements space out by "tab".
            </li>
            </ol>
        </div-content>
        <pre>
    Fastq file:
        @hiseq2000
        ATCGGCTAATCGGCTAATCGGCTA
        +
        iiiibbbibibibibibbbbiiib
    Line file:
        @hiseq2000  ATCGGCTAATCGGCTAATCGGCTA    +   iiiibbbibibibibibbbbiiib</pre>

    
<h3>
<a id="counter-options" class="anchor" href="#counter-options" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Reflexiv counter options</h3>
        
        <p>
            <i>Sparkhit mapper</i> is a short read aligner built on top of the <i>Apache Spark</i> platform. The options for <i>Sparkhit mapper</i> are similar to standard alignment tools: an input file of sequencing data (usually a fastq file); An input reference genome; An output result file.
        </p>
        <p>
            Different from the Sparkhit-recruiter, the mapper runs much faster than the recruiter by using the Pigeonhole principle as a more strict filter. Thus, Sparkhit-mapper is recommanded in the situation where only high quality matches (more than 94% identity) are needed for the analysis.</p>
        <p>
            To list all options, simply type command: <br>
            <code>$ sparkhit mapper</code>
        </p>       
        <pre>
SparkHit 15:32:41 SparkHit main initiating ... 
SparkHit 15:32:41 interpreting parameters.
Name:
	SparkHit mapper

Options:                             
  -fastq &lt;input fastq file>          Input Next Generation Sequencing (NGS) data,
                                     fastq file format, four line per unit
  -line &lt;input line file>            Input NGS data, line based text file format, one
                                     line per unit
  -tag                               Set to tag filename to sequence id. It is useful
                                     when you are processing lots of samples at the
                                     same time
  -reference &lt;input reference>       Input genome reference file, usually fasta
                                     format file, as input file
  -outfile &lt;output file>             Output line based file in text format
  -kmer &lt;kmer size>                  Kmer length for reads mapping
  -evalue &lt;e-value>                  e-value threshold, default 10
  -global &lt;global or not>            Use global alignment or not. 0 for local, 1 for
                                     global, default 0
  -unmask &lt;unmask>                   whether mask repeats of lower case nucleotides:
                                     1: yes; 0 :no; default=1
  -overlap &lt;kmer overlap>            small overlap for long read
  -identity &lt;identity threshold>     minimal identity for recruiting a read, default
                                     75 (sensitive mode, fast mode starts from 94)
  -coverage &lt;coverage threshold>     minimal coverage for recruiting a read, default
                                     30
  -minlength &lt;minimal read length>   minimal read length required for processing
  -attempts &lt;number attempts>        maximum number of alignment attempts for one
                                     read to a block, default 20
  -hits &lt;hit number>                 how many hits for output: 0:all; N: top N hits
  -strand &lt;strand +/->
  -thread &lt;number of threads>        How many threads to use for parallelizing
                                     processes,default is 1 cpu. set to 0 is the
                                     number of cpus available!local mode only, for
                                     Spark version, use spark parameter!
  -partition &lt;re-partition number>   re generate number of partitions for .gz data,
                                     as .gz data only have one partition (spark
                                     parallelization)
  -version                           show version information
  -help                              print and show this information
  -h

Usage:
	run short read mapping : 
spark-submit [spark parameter] --class uni.bielefeld.cmg.sparkhit.main.MainOfMapper sparkhit.jar [parameters] -fastq query.fq -reference reference.fa -outfile output_file.txt
spark-submit [spark parameter] --class uni.bielefeld.cmg.sparkhit.main.MainOfMapper sparkhit.jar [parameters] -line query.txt -reference reference.fa -outfile output_file.txt
sparkhit [command] [spark parameter] [parameters] -fastq query.fq -reference reference.fa -outfile output_file</pre>
        <table>
            <caption>Table M-4:  Descriptions of Reflexiv counter options</caption>
  <tr>
    <th>Options</th>
    <th>Description</th> 
  </tr>
  <tr>
    <td>-fastq</td>
      <td>The input sequencing data is in the fastq format. The input files can be compressed or uncompressed. Users can also use the wild card or comma seperated files to input a batch of files. eg. <code>-fastq /vol/data/samples/*.fq.bz2</code>.
      </td> 
  </tr>
  <tr>
    <td>-line</td>
      <td>The input sequencing data is a fastq file in the line-based format. This line-based sequecing data is useful when users have uncompressed files distributed across different nodes. In this case, a fastq unit is intact on one worker node. See below <b>Notes</b>.
      </td> 
  </tr>
        <tr>
    <td>-tag</td>
            <td>Whether to tag the file name to each sequence or not. This tag function can mark each sequence of the intermediate mapping result. This information is useful when users have a list of samples input in the same time and would like to be summarized seperately. This option will increase the size of output file.
            </td> 
  </tr>    
  <tr>
    <td>-reference</td>
            <td>Input file of the reference genome in the fasta format. Make sure it is located in a shared file system or downloaded to each worker node.
            </td> 
  </tr>    
  <tr>
    <td>-outfile</td>
            <td>Directory of output files.
            </td> 
  </tr>
            <tr>
    <td>-kmer</td>
            <td>The K-mer length for building the reference index, 8-12 is recommanded. Default is 12.
            </td> 
  </tr>
            <tr>
    <td>-evalue</td>
            <td>Set a maximum E-value as a filter to remove mappings higher than the threshold.
            </td> 
  </tr>
            <tr>
    <td>-global</td>
            <td>Global alignment or local alignment. Set to 0 as local alignment, to 1 as global alignment. The default is 0.
            </td> 
  </tr>
            <tr>
    <td>-unmask</td>
            <td>Whether to mask repeats in the reference genome or not. 0 for not mask; 1 for mask. The default is 1.
            </td> 
  </tr>
            <tr>
    <td>-overlap</td>
            <td>When extracting k-mers from the reference genome, there will be a shift between k-mers. The k-mer length minus the shifted length is the overlap length. The larger the overlap size is, the more sensitive the mapping result will be.
            </td> 
  </tr>
            <tr>
    <td>-identity</td>
            <td>The mapping identity between the sequencing read and the reference genome. The identity is calculated by dividing perfectly matched nuclieotides to the length of the sequecing read.
            </td> 
  </tr>
            <tr>
    <td>-coverage</td>
            <td>The minimal coverage to recruit a read. Default is 30.
            </td> 
  </tr>
            <tr>
    <td>-minlength</td>
            <td>The minmal length to recruit a read.
            </td> 
  </tr>
            <tr>
    <td>-attempts</td>
            <td>Sparkhit mapper trys to map each sequence to a collection of q-Grams (a block of sequence that is very likely to be recuited). This parameter sets a number of failed attempts to map to q-Grams. Default is 20
            </td> 
  </tr>
         <tr>
    <td>-hits</td>
            <td>How many hits to be reported in the result for each sequencing read. 0 for report all possible matches. The default is 0.
            </td> 
  </tr>
            <tr>
    <td>-strand</td>
            <td>Strands selection. 0 is for mapping to both strands of reference genome. 1 for mapping to only "+" strand. 2 for mapping to only "-". The Default is 0.
            </td> 
  </tr>
            <tr>
    <td>-thread</td>
            <td>How many threads to be used in parallel. This parameter is deprecated. Check the <code>-partition</code>.
            </td> 
  </tr>
            <tr>
    <td>-partition</td>
            <td>Re-distribute the input data into a number of partitions. Each partition runs a individual mapping task. This is useful when you want to evenly distribute your data for parallel computations.
            </td> 
  </tr>
            <tr>
    <td>-version</td>
            <td>Print out the Sparkhit version.
            </td> 
  </tr>
            <tr>
    <td>-help</td>
            <td>Print the help information.
            </td> 
  </tr>
        </table>
        <p>
        </p>
        <div>
            &#9749; &nbsp;Notes
        </div>
        <div-content>
            <ol type="1">
            <li>
                Different from the Sparkhit-recruiter, the mapper enables a faster sequence alignment by implementing the pigeonhole principle. While increasing the mapping speed, Sparkhit-mapper is less tolerant on mismatches during the alignment, due to the nature of the pigeonhole principle. Thus, when users only need good quality matches (more than 94% identity), Sparkhit-mapper is recommanded.
            </li>
            </ol>
        </div-content>

<h3>
<a id="reassembler-options" class="anchor" href="#reassembler-options" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Reflexiv reassembler options</h3>
        
        <p>
            <i>Sparkhit</i> reporter is a tool to summarize the mapping result of <code>sparkhit mapper</code>. Like standard word count applictions, it applies a simple Map-Reduce computation to summarize the frequency of each element. In our case, it is the mapped read number of each chromosome/contig/scaffold of the reference genomes. 
        </p>
        <p>
            To list all options, type command: <br>
            <code>$ ./bin/reflexiv reassembler</code>
        </p>
        <pre>
SparkHit 11:48:21 SparkHit Reporter initiating ... 
SparkHit 11:48:21 interpreting parameters.
Name:
	SparkHit Reporter

Options:
  -input &lt;input sparkhit result file>   Input spark hit result file in tabular
                                        format. Accept wild card, s3n schema, hdfs
                                        schema
  -word &lt;columns for identifier>        a list of column number used to represent a
                                        category you want to summarize. eg, 1,3,8
                                        means counting column 1 (chr), column 3
                                        (strain), column 8 (identity)
  -count &lt;column for count number>      the number of column value which will be used
                                        to aggregate for the report. set to 0 the
                                        value will be 1 for every identifier
  -outfile &lt;output report file>         Output report file in text format
  -partition &lt;re-partition number>      re generate number of partitions for .gz
                                        data, as .gz data only have one partition
                                        (spark parallelization)
  -version                              show version information
  -help                                 print and show this information
  -h

Usage:
	Report mapping summary
spark-submit [spark parameter] --class uni.bielefeld.cmg.sparkhit.main.MainOfReporter Sparkhit.jar [parameters] -input ./sparkhit.out -outfile ./sparkhit.report
sparkhit reporter [spark parameter] [parameters] -input ./sparkhit.out -outfile ./sparkhit.report</pre>
        <p>
            
        </p>
          <table>
            <caption>Table M-5: Descriptitons of Reflexiv reassembler options</caption>
  <tr>
    <th>Options</th>
    <th>Description</th> 
  </tr>
  <tr>
    <td>-input</td>
      <td>Input files of the fragment recruitment result in the tabular format.
      </td> 
  </tr>
  <tr>
    <td>-word</td>
    <td>A list of column numbers used to represent a category you want to summarize. See <b>Notes</b> for details.</td> 
  </tr>
    <tr>
        <td>-count</td>
        <td>Users can specify a column as the count number. This is useful when the tabular format has one column already stating the number of matches. By default, it is always one read per line.</td>
    </tr>
    <tr>
        <td>-outfile</td>
        <td>The path of the output result file, tabular format.</td>
    </tr>
    <tr>
        <td>-partition</td>
        <td>Re-distribute the input data into a number of partitions. This is useful when you want to evenly distribute your data for parallel computations.</td>
    </tr>
    <tr>
        <td>-version</td>
        <td>Display the Sparkhit version.</td>
    </tr>
    <tr>
        <td>-help</td>
        <td>Print the help information.</td>
    </tr>

</table>
        <p>
        </p>
        <div>
            &#9749; &nbsp;Notes
        </div>
        <div-content>
            <ol type="1">
            <li>
                The reporter usually summarizes the mapped read number of a certain chromosome/contig/scaffold above a certain identity, eg. how many reads mapped to <code>NM_086618</code> with more than 98% identity. Some times users might want to make some changes, eg. how many reads map to the <code>NM_086618</code> "+" strand with more than 97% identity. We offer a flexible option for the summarization, the <code>-word</code> option. It allows you to specify the identifier with a combination of elements by selecting a list of columns. Each line of the input tabular format file is a hit of a sequencing read. By choosing and combinig several columns, users can assemble their identifier as a key word for thesummarization (like a MapReduce WordCount).
            </li>
            </ol>
        </div-content>
        
<h3>
<a id="setting-spark-sge" class="anchor" href="#setting-spark-sge" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Setup a Spark cluster on Sun Grid Engine (SGE)</h3>
        <p>
            To setup a Spark cluster on SGE, we can use scripts located in the <code>./sbin</code> folder where <i>Spark</i> is installed.
        </p>
        <p>
            We can, firstly, start a <i>Spark</i> master node at the SGE login node: <br>
            <code>sh start-master.sh</code> <br>
            Then, a success info should be displayed on you screen: <br>
        </p>
            <pre>
starting org.apache.spark.deploy.master.Master, logging to $SPARK_HOME/logs/spark-huanglr-org.apache.spark.deploy.master.Master-1-zorin.out</pre>
       <p>
           Open the log file to see the master node info: <br>
           <code>less $SPARK_HOME/logs/spark-huanglr-org.apache.spark.deploy.master.Master-1-zorin.out</code>
        </p>
        <pre>
Spark Command: /usr/lib/jvm/java-1.8.0/jre//bin/java -cp $SPARK_HOME/conf/:$SPARK_HOME/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host zorin --port 7077 --webui-port 8080
========================================
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/03/10 13:38:36 INFO Master: Started daemon with process name: 12018@zorin
17/03/10 13:38:36 INFO SignalUtils: Registered signal handler for TERM
17/03/10 13:38:36 INFO SignalUtils: Registered signal handler for HUP
17/03/10 13:38:36 INFO SignalUtils: Registered signal handler for INT
17/03/10 13:38:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/10 13:38:38 INFO SecurityManager: Changing view acls to: huanglr
17/03/10 13:38:38 INFO SecurityManager: Changing modify acls to: huanglr
17/03/10 13:38:38 INFO SecurityManager: Changing view acls groups to: 
17/03/10 13:38:38 INFO SecurityManager: Changing modify acls groups to: 
17/03/10 13:38:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(huanglr); groups with view permissions: Set(); users  with modify permissions: Set(huanglr); groups with modify permissions: Set()
17/03/10 13:38:39 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
17/03/10 13:38:39 INFO Master: Starting Spark master at spark://zorin:7077
17/03/10 13:38:39 INFO Master: Running Spark version 2.0.0
17/03/10 13:38:40 INFO Utils: Successfully started service 'MasterUI' on port 8080.
17/03/10 13:38:40 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://zorin:8080
17/03/10 13:38:40 INFO Utils: Successfully started service on port 6066.
17/03/10 13:38:40 INFO StandaloneRestServer: Started REST server for submitting applications on port 6066
17/03/10 13:38:41 INFO Master: I have been elected leader! New state: ALIVE</pre>
        <p>
            Now, you can browse the WebUI of the master node via: <br>
            <code>http://zorin:8080</code>, where <code>zorin</code> is the hostname of the master node.
        </p>
        <p>
            Next, we will add worker nodes to the master node. To do that, we use the <code>start-slave.sh</code> script located at the <code>./sbin</code> folder where <i>Spark</i> is installed.<br>
            To submit a slave deamon to the SGE compute node, we can write a script for the SGE submission:
        </p>
        <pre>
#!/bin/sh

# Set SGE options:
## run the job in the current working directory (where qsub is called)
#$ -cwd
## Specify an architecture
#$ -l arch=lx24-amd64
## Specify multislot pe
#$ -pe multislot 48
## Specify memory for each pe job
#$ -l vf=2G

sh $SPARK_HOME/sbin/start-slave.sh spark://zorin:7077

sleep 100000000</pre>
        <p>
            Where <code>spark://zorin:7077</code> is used for assigning this worker node to the master node we launched before. The code <code>sleep 1000000</code> is to keep the deamon running on the submitted SGE compute node.
        </p>
        <p>
            Check the log file again to see if the worker node connection is successful: <br>
           <code>less $SPARK_HOME/logs/spark-huanglr-org.apache.spark.deploy.master.Master-1-zorin.out</code>
        </p>
        <pre>
17/03/10 13:38:40 INFO Utils: Successfully started service on port 6066.
17/03/10 13:38:40 INFO StandaloneRestServer: Started REST server for submitting applications on port 6066
17/03/10 13:38:41 INFO Master: I have been elected leader! New state: ALIVE
17/03/10 14:03:58 INFO Master: Registering worker statler:45341 with 48 cores, 250.8 GB RAM</pre>
        <p>
            To shutdown worker nodes, simply use <code>qdel</code> command to delete submitted jobs.
        </p>
        <div>
            &#9749; &nbsp;Notes
        </div>
        <div-content>
            <ol type="1">
            <li>
                To launch a group of worker nodes, use the <code>start-slaves.sh</code> script located in the <code>./sbin</code> folder. Before running the <code>start-slaves.sh</code> script, include all IP addresses of the compute nodes to the <code>./conf/slaves</code> file in <code>$SPARK_HOME</code>.
            </li>
            </ol>
        </div-content>
        
<h3>
<a id="setting-spark-ec2" class="anchor" href="#setting-spark-ec2" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Setup a Spark cluster on the AWS EC2 cloud</h3>
        <p>
            We use Spark-ec2 to setup a Spark cluster on the Amazon EC2 cloud. Detailed instructions can be found at <a href="https://github.com/amplab/spark-ec2">Spark-ec2</a>. Here, we provide an example to setup a 5-node Spark cluster.
        </p>
        <p>
            To view all options: <br>
            <code>$ $SPARK_HOME/ec2/spark-ec2</code>
        </p>
       
        <pre>
Usage: spark-ec2 [options] &lt;action> &lt;cluster_name>

&lt;action> can be: launch, destroy, login, stop, start, get-master, reboot-slaves

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -s SLAVES, --slaves=SLAVES
                        Number of slaves to launch (default: 1)
  -w WAIT, --wait=WAIT  DEPRECATED (no longer necessary) - Seconds to wait for
                        nodes to start
  -k KEY_PAIR, --key-pair=KEY_PAIR
                        Key pair to use on instances
  -i IDENTITY_FILE, --identity-file=IDENTITY_FILE
                        SSH private key file to use for logging into instances
  -p PROFILE, --profile=PROFILE
                        If you have multiple profiles (AWS or boto config),
                        you can configure additional, named profiles by using
                        this option (default: none)
  -t INSTANCE_TYPE, --instance-type=INSTANCE_TYPE
                        Type of instance to launch (default: m1.large).
                        WARNING: must be 64-bit; small instances won't work
  -m MASTER_INSTANCE_TYPE, --master-instance-type=MASTER_INSTANCE_TYPE
                        Master instance type (leave empty for same as
                        instance-type)
  -r REGION, --region=REGION
                        EC2 region used to launch instances in, or to find
                        them in (default: us-east-1)
  -z ZONE, --zone=ZONE  Availability zone to launch instances in, or 'all' to
                        spread slaves across multiple (an additional $0.01/Gb
                        for bandwidthbetween zones applies) (default: a single
                        zone chosen at random)
  -a AMI, --ami=AMI     Amazon Machine Image ID to use
  -v SPARK_VERSION, --spark-version=SPARK_VERSION
                        Version of Spark to use: 'X.Y.Z' or a specific git
                        hash (default: 1.6.0)
  --spark-git-repo=SPARK_GIT_REPO
                        Github repo from which to checkout supplied commit
                        hash (default: https://github.com/apache/spark)
  --spark-ec2-git-repo=SPARK_EC2_GIT_REPO
                        Github repo from which to checkout spark-ec2 (default:
                        https://github.com/amplab/spark-ec2)
  --spark-ec2-git-branch=SPARK_EC2_GIT_BRANCH
                        Github repo branch of spark-ec2 to use (default:
                        branch-1.5)
  --deploy-root-dir=DEPLOY_ROOT_DIR
                        A directory to copy into / on the first master. Must
                        be absolute. Note that a trailing slash is handled as
                        per rsync: If you omit it, the last directory of the
                        --deploy-root-dir path will be created in / before
                        copying its contents. If you append the trailing
                        slash, the directory is not created and its contents
                        are copied directly into /. (default: none).
  --hadoop-major-version=HADOOP_MAJOR_VERSION
                        Major version of Hadoop. Valid options are 1 (Hadoop
                        1.0.4), 2 (CDH 4.2.0), yarn (Hadoop 2.4.0) (default:
                        1)
  -D [ADDRESS:]PORT     Use SSH dynamic port forwarding to create a SOCKS
                        proxy at the given local address (for use with login)
  --resume              Resume installation on a previously launched cluster
                        (for debugging)
  --ebs-vol-size=SIZE   Size (in GB) of each EBS volume.
  --ebs-vol-type=EBS_VOL_TYPE
                        EBS volume type (e.g. 'gp2', 'standard').
  --ebs-vol-num=EBS_VOL_NUM
                        Number of EBS volumes to attach to each node as
                        /vol[x]. The volumes will be deleted when the
                        instances terminate. Only possible on EBS-backed AMIs.
                        EBS volumes are only attached if --ebs-vol-size > 0.
                        Only support up to 8 EBS volumes.
  --placement-group=PLACEMENT_GROUP
                        Which placement group to try and launch instances
                        into. Assumes placement group is already created.
  --swap=SWAP           Swap space to set up per node, in MB (default: 1024)
  --spot-price=PRICE    If specified, launch slaves as spot instances with the
                        given maximum price (in dollars)
  --ganglia             Setup Ganglia monitoring on cluster (default: True).
                        NOTE: the Ganglia page will be publicly accessible
  --no-ganglia          Disable Ganglia monitoring for the cluster
  -u USER, --user=USER  The SSH user you want to connect as (default: root)
  --delete-groups       When destroying a cluster, delete the security groups
                        that were created
  --use-existing-master
                        Launch fresh slaves, but use an existing stopped
                        master if possible
  --worker-instances=WORKER_INSTANCES
                        Number of instances per worker: variable
                        SPARK_WORKER_INSTANCES. Not used if YARN is used as
                        Hadoop major version (default: 1)
  --master-opts=MASTER_OPTS
                        Extra options to give to master through
                        SPARK_MASTER_OPTS variable (e.g
                        -Dspark.worker.timeout=180)
  --user-data=USER_DATA
                        Path to a user-data file (most AMIs interpret this as
                        an initialization script)
  --authorized-address=AUTHORIZED_ADDRESS
                        Address to authorize on created security groups
                        (default: 0.0.0.0/0)
  --additional-security-group=ADDITIONAL_SECURITY_GROUP
                        Additional security group to place the machines in
  --additional-tags=ADDITIONAL_TAGS
                        Additional tags to set on the machines; tags are
                        comma-separated, while name and value are colon
                        separated; ex: "Task:MySparkProject,Env:production"
  --copy-aws-credentials
                        Add AWS credentials to hadoop configuration to allow
                        Spark to access S3
  --subnet-id=SUBNET_ID
                        VPC subnet to launch instances in
  --vpc-id=VPC_ID       VPC to launch instances in
  --private-ips         Use private IPs for instances rather than public if
                        VPC/subnet requires that.
  --instance-initiated-shutdown-behavior=INSTANCE_INITIATED_SHUTDOWN_BEHAVIOR
                        Whether instances should terminate when shut down or
                        just stop
  --instance-profile-name=INSTANCE_PROFILE_NAME
                        IAM profile name to launch instances under
        </pre>
        <p>
            To setup a Spark cluster, we have to specify the master node and the number of worker nodes with options <code>-m</code> and <code>-s</code>. If you want to use spot instances, set the option <code>--spot-price=0.8</code>, where 0.8 is the bid price in dollar. </p>
        <p>
            You also have to set your correspond AWS security Key with <code>-k rhinempi</code> and <code>-i /vol/AWS/Key/rhinempi.pem</code>. Here, <i>rhinempi</i> is the name of my security Key.
        </p> 
        <p>
            Also include your <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> to your ENV:<br>
            <code>$ vi ~/.bash_profile</code>
        </p>
        <pre>
# --- spark ec2 env --- #
export AWS_ACCESS_KEY_ID="xxxxxxxxxx"
export AWS_SECRET_ACCESS_KEY="xxxxxxxxxxx"</pre>
        <p>
            Update your ENV:<br>
            <code>source ~/.bash_profile</code>
        </p>
        Here is an example command:
        <p>
            <code>
                $ $SPARK_HOME/ec2/spark-ec2 -m m1.xlarge -k rhinempi -i /vol/AWS/Key/rhinempi.pem -s 5 --spot-price=0.8 --instance-type=c3.8xlarge --region=eu-west-1 launch Map-All-HMP
            </code>
        </p>
             
        <pre>
Setting up security groups...
Searching for existing cluster Map-All-HMP in region eu-west-1...
Spark AMI: ami-1ae0166d
Launching instances...
Requesting 5 slaves as spot instances with price $0.800
Waiting for spot instances to be granted...
All 5 slaves granted
Launched master in eu-west-1a, regid = r-0cd1d32a495250e99
Waiting for AWS to propagate instance metadata...
Waiting for cluster to enter 'ssh-ready' state......</pre>
<p>
        To terminate the cluster, use the <code>destroy</code> command of Spark-ec2:
        </p>
        <p>
            <code>$ $SPARK_HOME/ec2/spark-ec2 --region=eu-west-1 destroy Map-All-HMP</code>
        </p>
        <pre>
Searching for existing cluster Map-All-HMP in region eu-west-1...
Found 1 master, 5 slaves.
The following instances will be terminated:
> ec2-54-194-240-108.eu-west-1.compute.amazonaws.com
> ec2-54-154-111-11.eu-west-1.compute.amazonaws.com
> ec2-54-154-69-167.eu-west-1.compute.amazonaws.com
> ec2-54-229-5-191.eu-west-1.compute.amazonaws.com
> ec2-54-194-140-87.eu-west-1.compute.amazonaws.com
> ec2-54-154-186-102.eu-west-1.compute.amazonaws.com
ALL DATA ON ALL NODES WILL BE LOST!!
Are you sure you want to destroy the cluster huanglrHMP? (y/N) y
Terminating master...
Terminating slaves...</pre>
        <div>
            &#9749; &nbsp;Notes
        </div>
        <div-content>
            <ol type="1">
            <li>
                For the Spark 2.0.0 version, the spark-ec2 script is not included in the package distribution. To download <code>spark-ec2</code>, use the following link <code>https://github.com/amplab/spark-ec2/archive/branch-2.0.zip</code>.
            </li>
            </ol>
        </div-content>
        
        <p>
            <a href="./manual.html">Back to top</a>
        </p>
        
      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/rhinempi/Reflexiv">Reflexiv</a> is maintained by <a href="https://github.com/rhinempi">Liren Huang</a>.</span>

        <span class="site-footer-credits">This page was generated based on a template from <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
